{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How we can create AI that makes optimal decisions given limited information and uncertainty.\n",
    "\n",
    "- Uncertainty can be represented as a number of events and the likelihood, or probability, of each of them happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Die example**\n",
    "- Every possible situation can be thought of as a world, represented by the lowercase Greek letter omega ω. \n",
    "\n",
    "- Rolling a die can result in six possible worlds: a world where the die yields a 1, a world where the die yields a 2, and so on. \n",
    "\n",
    "- To represent the probability of a certain world, we write P(ω).\n",
    "\n",
    "#### Axioms in Probability\n",
    "\n",
    "- `0 < P(ω) < 1:` every value representing probability must range between 0 and 1.\n",
    "\n",
    "    - **Zero is an impossible event**, like rolling a standard die and getting a 7.\n",
    "    \n",
    "    - **One is a certain event**, like rolling a standard die and getting a value of 10.\n",
    "\n",
    "- In general, the higher the value, the more likely the event is to happen.\n",
    "\n",
    "- The probability of rolling a number R with a standard die can be represented as P(R).\n",
    "\n",
    "- In our case, P(R) = 1/6, because there are six possible worlds (rolling any number from 1 through 6) and each is equally likely to happen. \n",
    "\n",
    "- The probabilities of every possible event, when summed together, are equal to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unconditional Probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Unconditional probability is the degree of belief in a proposition in the absence of any other evidence. \n",
    "\n",
    "- Like the result of rolling a die is not dependent on previous events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Conditional probability is the degree of belief in a proposition given some evidence that has already been revealed. \n",
    "\n",
    "- AI can use partial information to make educated guesses about the future. \n",
    "\n",
    "    - To use this information, which affects the probability that the event occurs in the future, we rely on conditional probability.\n",
    "\n",
    "- Conditional probability is expressed using the following notation: \n",
    "\n",
    "    - `P(a | b)`, meaning “the probability of event a occurring given that we know event b to have occurred,” or, more succinctly, “the probability of a given b.” \n",
    "\n",
    "#### Conditional Probability Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A random variable is a variable in probability theory with a domain of possible values that it can take on. \n",
    "\n",
    "- For example, to represent possible outcomes when rolling a die, we can define a random variable Roll, that can take on the values {0, 1, 2, 3, 4, 5, 6}. \n",
    "\n",
    "\n",
    "#### Independence\n",
    "\n",
    "- Independence is the knowledge that the occurrence of one event does not affect the probability of the other event. \n",
    "\n",
    "- For example, when rolling two dice, the result of each die is independent from the other. Rolling a 4 with the first die does not influence the value of the second die that we roll. \n",
    "\n",
    "- This is opposed to dependent events, like clouds in the morning and rain in the afternoon, so these events are dependent.\n",
    "\n",
    "Independence can be defined mathematically: `P(a ∧ b) = P(a)P(b)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes’ Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bayes’ rule says that the probability of b given a is equal to the probability of a given b, times the probability of b, divided by the probability of a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Knowing P(a | b), in addition to P(a) and P(b), allows us to calculate P(b | a). \n",
    "\n",
    "- This is helpful, because knowing the conditional probability of a visible effect given an unknown cause, P(visible effect | unknown cause), \n",
    "\n",
    "- allows us to calculate the probability of the unknown cause given the visible effect, P(unknown cause | visible effect)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Joint probability is the likelihood of multiple events all occurring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negation: P(¬a) = 1 - P(a).** \n",
    "\n",
    "- This stems from the fact that the sum of the probabilities of all the possible worlds is 1, and the complementary literals a and ¬a include all the possible worlds.\n",
    "\n",
    "**Inclusion-Exclusion: P(a ∨ b) = P(a) + P(b) - P(a ∧ b).**\n",
    "\n",
    "- The worlds in which a or b are true are equal to all the worlds where a is true, plus the worlds where b is true.\n",
    "\n",
    "- However, in this case, some worlds are counted twice (the worlds where both a and b are true). \n",
    "\n",
    "- To get rid of this overlap, we subtract once the worlds where both a and b are true.\n",
    "\n",
    "**Marginalization: P(a) = P(a, b) + P(a, ¬b).** \n",
    "\n",
    "- The idea here is that b and ¬b are disjoint probabilities. \n",
    "\n",
    "- That is, the probability of b and ¬b occurring at the same time is 0. We also know b and ¬b sum up to 1. \n",
    "\n",
    "- Thus, when a happens, b can either happen or not. \n",
    "\n",
    "- When we take the probability of both a and b happening in addition to the probability of a and ¬b, we end up with simply the probability of a.\n",
    "\n",
    "\n",
    "**Conditioning: P(a) = P(a | b)P(b) + P(a | ¬b)P(¬b).** \n",
    "\n",
    "- The probability of event a occurring is equal to the probability of a given b times the probability of b, plus the probability of a given ¬b time the probability of ¬b.\n",
    "Conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Bayesian network is a data structure that represents the dependencies among random variables. \n",
    "\n",
    "- Bayesian networks have the following properties:\n",
    "\n",
    "    - They are directed graphs.\n",
    "    \n",
    "    - Each node on the graph represent a random variable.\n",
    "    \n",
    "    - An arrow from X to Y represents that X is a parent \n",
    "    of Y. That is, the probability distribution of Y depends on the value of X.\n",
    "    - Each node X has probability distribution P(X | \n",
    "    Parents(X))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We could definitively conclude new information based on the information that we already had. \n",
    "\n",
    "- We can also infer new information based on probabilities. \n",
    "\n",
    "- While this does not allow us to know new information for certain, it allows us to figure out the probability distributions for some values.\n",
    "\n",
    "**Inference by Enumeration**\n",
    "\n",
    "Inference by enumeration is a process of finding the probability distribution of variable X given observed evidence e and some hidden variables Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sampling is one technique of approximate inference. \n",
    "\n",
    "- In sampling, each variable is sampled for a value according to its probability distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In many tasks do rely on the dimension of time, such as prediction. \n",
    "\n",
    "- To represent the variable of time we will create a new variable, X, and change it based on the event of interest, such that Xₜ is the current event, Xₜ₊₁ is the next event, and so on. \n",
    "\n",
    "- To be able to predict events in the future, we will use Markov Models.\n",
    "\n",
    "- The **Markov assumption** is an assumption that the current state depends on only a finite fixed number of previous states. \n",
    "\n",
    "- A **Markov chain** is a sequence of random variables where the distribution of each variable follows the Markov assumption. \n",
    "    \n",
    "    - That is, each event in the chain occurs based on the probability of the event before it.\n",
    "\n",
    "- A **hidden Markov model** is a type of a Markov model for a system with hidden states that generate some observed event. \n",
    "\n",
    "    - This means that sometimes, the AI has some measurement of the world but no access to the precise state of the world. \n",
    "    \n",
    "    - In these cases, the state of the world is called the hidden state and whatever data the AI has access to are the observations. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
